{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncq4PM_udeOi"
      },
      "source": [
        "# Headers and Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "y6eHSx4MunrG"
      },
      "outputs": [],
      "source": [
        "#!pip install transformers datasets peft accelerate\n",
        "!pip install bitsandbytes datasets trl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXTAOeEzwMqb"
      },
      "outputs": [],
      "source": [
        "# load dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGH63VxLc0Ay"
      },
      "source": [
        "# Finetuning the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAUoVWq0dIcD"
      },
      "source": [
        "**Loading the Model and Tokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OTTb8iX2c1Wh"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from trl import SFTConfig, SFTTrainer, setup_chat_format, DataCollatorForCompletionOnlyLM\n",
        "import torch\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model_name = \"aisingapore/gemma2-9b-cpt-sea-lionv3-instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    pretrained_model_name_or_path=model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFgFLj7PdSOa"
      },
      "source": [
        "**Loading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtjOqeb2c1nG"
      },
      "outputs": [],
      "source": [
        "file_path = \"/content/drive/MyDrive/TweetTaglish/TweetTaglish-parallel.csv\"\n",
        "\n",
        "# Load the CSV file\n",
        "raw_dataset = load_dataset(\"csv\", data_files={\"train\": file_path}, split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-FUMG2yf4X4"
      },
      "outputs": [],
      "source": [
        "# Add prompt to finetuning data so the model understands what it's looking at\n",
        "def reformat(example):\n",
        "  example = {\n",
        "      \"prompt\": [{\"role\": \"user\", \"content\": f\"Translate the following Tweet from English to Tagalog-English code-switching:\\n {example['input_text']}\"}],\n",
        "      \"completion\": [{\"role\": \"assistant\", \"content\": example['target_text']}]\n",
        "      }\n",
        "\n",
        "  return example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNT23Pgcg0jN"
      },
      "outputs": [],
      "source": [
        "raw_dataset = raw_dataset.map(reformat, remove_columns=raw_dataset.column_names)\n",
        "raw_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ZZ6sH6lhrK_"
      },
      "source": [
        "**LoRA Config**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0MaZ_bvXhtxv"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig\n",
        "\n",
        "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
        "rank_dimension = 16\n",
        "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
        "lora_alpha = 8\n",
        "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
        "lora_dropout = 0.05\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
        "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
        "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
        "    bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
        "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
        "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8IK_5qkhwsY"
      },
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "# Hyperparameters based on QLoRA paper recommendations\n",
        "args = SFTConfig(\n",
        "    max_seq_length=4096,\n",
        "    packing=False,\n",
        "\n",
        "    # Output settings\n",
        "    output_dir=\"./lora-sealion-finetuned\",  # Directory to save model checkpoints\n",
        "\n",
        "    # Training duration\n",
        "    num_train_epochs=3,  # Number of training epochs\n",
        "\n",
        "    # Batch size settings\n",
        "    per_device_train_batch_size=4,  # Batch size per GPU\n",
        "    gradient_accumulation_steps=4,  # Accumulate gradients for larger effective batch\n",
        "\n",
        "    # Memory optimization\n",
        "    gradient_checkpointing=True,  # Trade compute for memory savings\n",
        "\n",
        "    # Optimizer settings\n",
        "    optim=\"adamw_torch_fused\",  # Use fused AdamW for efficiency\n",
        "    learning_rate=2e-4,  # Learning rate (QLoRA paper)\n",
        "    max_grad_norm=0.3,  # Gradient clipping threshold\n",
        "\n",
        "    # Learning rate schedule\n",
        "    warmup_ratio=0.03,  # Portion of steps for warmup\n",
        "    lr_scheduler_type=\"constant\",  # Keep learning rate constant after warmup\n",
        "\n",
        "    # Logging and saving\n",
        "    logging_steps=10,  # Log metrics every N steps\n",
        "    save_strategy=\"epoch\",  # Save checkpoint every epoch\n",
        "\n",
        "    # Precision settings\n",
        "    bf16=True,  # Use bfloat16 precision\n",
        "\n",
        "    # Integration settings\n",
        "    push_to_hub=False,  # Don't push to HuggingFace Hub\n",
        "    report_to=\"none\",  # Disable external logging\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOhF5t7ghzx6"
      },
      "outputs": [],
      "source": [
        "# Create SFTTrainer with LoRA configuration\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=raw_dataset,\n",
        "    peft_config=peft_config,  # LoRA configuration\n",
        "    processing_class=tokenizer\n",
        "    # data_collator=collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hlDVJpoh1eU"
      },
      "outputs": [],
      "source": [
        "train_dataloader = trainer.get_train_dataloader()\n",
        "\n",
        "index = 0\n",
        "for batch_data in train_dataloader:\n",
        "    input_ids = batch_data['input_ids'][index]\n",
        "    attention_mask = batch_data['attention_mask'][index]\n",
        "    label_ids = batch_data['labels'][index]\n",
        "\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    decoded = tokenizer.decode(input_ids, skip_special_tokens=False)\n",
        "\n",
        "    print(\"Tokens:\")\n",
        "    for i, token in enumerate(tokens):\n",
        "        attn = attention_mask[i]\n",
        "        if label_ids[i] != -100:\n",
        "          label_token = tokenizer.convert_ids_to_tokens([label_ids[i]])[0]\n",
        "        else:\n",
        "          label_token = 'IGN'\n",
        "\n",
        "        print(f\"{i:2d}: {token:12s} | Label_id: {label_ids[i]} | Attention: {attn} | Label: {label_token}\")\n",
        "\n",
        "    print(\"\\nDecoded sentence:\")\n",
        "    print(decoded)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "w0Jca-qxxTa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# After training with SFTTrainer\n",
        "trainer.model.save_pretrained(\"lora-sealion-finetuned-1\")\n",
        "\n",
        "# Merge and save full weights\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"aisingapore/gemma2-9b-cpt-sea-lionv3-instruct\",\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "merged_model = PeftModel.from_pretrained(base_model, \"lora-sealion-finetuned-1\")\n",
        "merged_model = merged_model.merge_and_unload()"
      ],
      "metadata": {
        "id": "vG_sXiy6Ybb_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Save model to Hugging Face"
      ],
      "metadata": {
        "id": "_PC--uSpD20g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model to Hugging Face\n",
        "from huggingface_hub import login\n",
        "login(token=\"\") #deleted\n",
        "\n",
        "merged_model.push_to_hub(\"charlottepuopolo/sealion-3v-9b-it-taglish\")\n",
        "tokenizer.push_to_hub(\"charlottepuopolo/sealion-3v-9b-it-taglish\")"
      ],
      "metadata": {
        "id": "UdDQy92tXwUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference"
      ],
      "metadata": {
        "id": "toT7m3y1JCAd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**inference**"
      ],
      "metadata": {
        "id": "ByUY2kjYzjPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "GubG1tzhUU6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline(\"text-generation\", model=\"charlottepuopolo/sealion-3v-9b-it-taglish\")"
      ],
      "metadata": {
        "id": "Wu21ybnAg0EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Translate the following Tweet from English to Tagalog-English code-switching:\\nHey How are you? Today has been crazy omg\"},\n",
        "]\n",
        "pipe(messages)\n"
      ],
      "metadata": {
        "id": "GUloyGZog8Zv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "Ncq4PM_udeOi",
        "GfbsPX0hcxja"
      ],
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}